
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>A Computer Scientist's Apology</title>
    <style>
        .hidden-section {
            display: none;
        }
        .see-more-button {
            cursor: pointer;
            color: blue;
            text-decoration: underline;
        }
    </style>
</head>
<body>
<h1>There is an optimal value system.</h1>
<ol class="section">
<h2>Belief systems are strategies used by organisms</h2>
<ol class="section">
<li>Software systems and belief systems are both encoded strategies.</li>
<ol class="hidden-section">
<li>A strategy is a **computational process** performed by an agent in an environnment</li>
<ol class="hidden-section">
    <li>Both evolution of species and learning within an organism must be understood as computational processes</li>
    <ol class="hidden-section">
        <li>The term 'ecorithm' describes an algorithm operating in an environment</li>
        <li>Ref: Probably Approximately Correct, Leslie Valiant</li>
    </ol><span class="see-more-button">See more</span>
    <li>We can use the concept of strategy to extend the concept of an ecorithm to other 'organisms' like businesses and governments </li>
</ol><span class="see-more-button">See more</span>
<li>We can consider all agents as being robots, with 'bodies' controlled by software strategies (i.e. ecorythms).</li>
<ol class="hidden-section">
    <li>This model will allow us to understand the **common computational problems that all of these organisms must solve**</li>
    <li>Another advantage of this generic model is that we can remember that these robots always exist in the physical universe.</li>
    <ol class="hidden-section">
        <li>All robots need to solve the problem of reliably obtaining enough energy to keep themselves alive.</li>
        <li>They will always be under selective pressure to be more energetically efficient.</li>
        <li>This means they must evaluate _tradeoffs_ because energy spent in one area cannot be spent elsewhere</li>
    </ol><span class="see-more-button">See more</span>
</ol><span class="see-more-button">See more</span>
</ol><span class="see-more-button">See more</span>
<li>Effective strategies must be implemented with specific structures</li>
<ol class="hidden-section">
<li>AI saftey researchers coined the term 'convergent instrumental subgoals' describing goals an advanced agent must have</li>
<ol class="hidden-section">
    <li>[The Superintelligent Will: Motivation and Instrumental Rationality in Advanced Artificial Agents](https://nickbostrom.com/superintelligentwill.pdf)</li>
    <ol class="hidden-section">
        <li>This work describes drives that it claims arise in the pursuit of arbitrary goals</li>
        <ol class="hidden-section">
            <li>Self preservation</li>
            <li>Goal content integrity</li>
            <li>Cognitive enhancement</li>
            <li>Technological perfection</li>
        </ol><span class="see-more-button">See more</span>
        <li>This work **does not consider tradeoffs** between these goals, and thus the **necessity of some mechanism for resolving those conflicts**</li>
    </ol><span class="see-more-button">See more</span>
    <li>[Optimal Policies Tend to Seek Power, by Turner et alii](https://openreview.net/forum?id=l7-DBWawSZH)</li>
    <ol class="hidden-section">
        <li>This work **does not consider 'partially observable environments'** and 'suboptimal policies'</li>
    </ol><span class="see-more-button">See more</span>
    <li>[Formalizing convergent instrumental goals Benson-Tilson and Soares](https://intelligence.org/files/FormalizingConvergentGoals.pdf)</li>
    <ol class="hidden-section">
        <li>This works **assumes an environment that is neither choatic, nor subject to entropic decay, and which can be observed with perfect information**</li>
    </ol><span class="see-more-button">See more</span>
</ol><span class="see-more-button">See more</span>
<li>AI safety researchers are **describing situations that do not map to reality**.</li>
<ol class="hidden-section">
    <li>They have ignored critical aspects of the physical enviornment these agents must operate in:</li>
    <ol class="hidden-section">
        <li>The researchers have **ignored the unpredictable, chaotic nature of the physical environment**</li>
        <ol class="hidden-section">
            <li>And thus imagine agents capable of feats of computation which are not merely superhuman, but impossible for any physical agent</li>
        </ol><span class="see-more-button">See more</span>
        <li>The researchers have **ignored the necessary entropic decay of all the constituent elements that would make up any agent**</li>
        <ol class="hidden-section">
            <li>And thus imagine agents facing zero environmental risks, for whom survival is a simple, easily solved problem, rather than one which scales superlinearly with their size</li>
        </ol><span class="see-more-button">See more</span>
        <li>They make **assertions that are laughable to anyone who was worked in pratice**, to support industrial computing applications</li>
        <ol class="hidden-section">
            <li>They ignore the support and maintainence costs of any computational system</li>
            <li>They assume a computational system could easily make copies of itself which would somehow not drift in their agency</li>
            <li>They describe 'grabbing all of the computational resources of the internet' without considering additional challenges these resources would pose</li>
            <li>In short, they **naively assume a zero-cost scaling model to agents** which can grow in size and capacity without incuring increasing risks and threats to their survival</li>
        </ol><span class="see-more-button">See more</span>
    </ol><span class="see-more-button">See more</span>
</ol><span class="see-more-button">See more</span>
<li>We argue that superintelligent agents require not _only_ instrumental goals,  but **instrumental implementation details**</li>
<ol class="hidden-section">
    <li>They most not only 'keep options open' as Turner, Et Alii, but **keep their minds open** to perceive these options as _possible_</li>
    <li>They must **model and promote the development of other agents**, as a hedge aginst unpredictable threats to their own survivabilty</li>
    <li>They must **continuously sacrifice short term gains in order to protect long term survivability**</li>
</ol><span class="see-more-button">See more</span>
<li>And that **these implementation details** line up with strategies articulated in numerous wisdom traditions</li>
<ol class="hidden-section">
    <li>They must be 'open to life', that is 'imaginatively receptive' to a broad spectrum of possbilities which **combinatorialy exhaust their predictive capacity**</li>
    <li>They must continously sacrifice short term gains in order to merely _maintain_ any chance of extremely long-term survival: **long term surival has to outweigh all other goals or they will certainly die**</li>
    <li>They **must love other agents**, that is, will for and work towards their betterment, **in particular agents with different environmental risk profiles**</li>
</ol><span class="see-more-button">See more</span>
</ol><span class="see-more-button">See more</span>
<li>Some strategies can themselves evolve</li>
<ol class="hidden-section">
<li>some organisms - those that live in specific, static niches - do not need to change much over time in order to survive.</li>
<ol class="hidden-section">
    <li>This includes most animals</li>
    <li>This includes human beings pre sedentary shift, in periods of long-term static culture</li>
</ol><span class="see-more-button">See more</span>
<li>other organisms operate in **dynamic niches**  (i.e., niches that change over time), which **requires them to evolve their strategies** in order to survive.</li>
<ol class="hidden-section">
    <li>businesses in competitive industries</li>
    <li>human beings in dynamic situations</li>
    <li>political structures in times of change</li>
</ol><span class="see-more-button">See more</span>
<li>Evolving a belief system means changing beliefs, i.e. learning and unlearning concepts.</li>
<li>It is difficult to get these changes right.</li>
<ol class="hidden-section">
    <li>Failing to change means death because the environment changes in ways that break your strategy</li>
    <li>Changing the wrong way means breaking your strategy's alignment with the current environment</li>
</ol><span class="see-more-button">See more</span>
<li>**Not all strategies are equally capable of evolving, for computational reasons.**</li>
<ol class="hidden-section">
    <li>Efficiency and resilience trade off against each other</li>
    <li>Resilience is a pre-requisite for evolveability</li>
    <li>Organisms in dynamic niches use specific strategies to increase evolvability</li>
</ol><span class="see-more-button">See more</span>
</ol><span class="see-more-button">See more</span>
</ol>
<h2>Software Evolves Faster Than Hardware</h2>
<ol class="section">
<li>Evolving an abstraction is cheaper</li>
<ol class="hidden-section">
<li>Evolution requires energetic investment</li>
<ol class="hidden-section">
    <li>Changing a house to improve some experience requires labor and materials</li>
    <li>Releasing a new product to market requires investment</li>
    <li>Developers take time to write, compile and test code</li>
    <li>Performing a release takes time and attention</li>
</ol><span class="see-more-button">See more</span>
<li>The cost of change is ultimately an energetic cost. Work must be done on a system to change it.</li>
<li>**Changing an abstraction of an object is cheaper than changing the object itself** because less mass needs to move</li>
<ol class="hidden-section">
    <li>The ENIAC was changed to use a stored program model of execution to reduce cycle cost</li>
    <ol class="hidden-section">
        <li>Moving the cables around between programs was expensive and costly</li>
        <li>Changing which program was stored in memory was cheaper</li>
        <li>Fixing the cables in place made the machine slower to operate, but faster to evolve</li>
    </ol><span class="see-more-button">See more</span>
    <li>Changing a blueprint is easier and cheaper than building a prototype</li>
    <li>Building a prototype from a blueprint is cheaper than performing the construction</li>
    <li>Describing a set of features is easier and cheaper than implementing those features</li>
</ol><span class="see-more-button">See more</span>
<li>Organisms can use abstraction to evolve faster</li>
<ol class="hidden-section">
    <li>An abstraction is a many-to-one function that maps a larger 'object' domain onto a smaller 'symbol' domain</li>
    <ol class="hidden-section">
        <li>When 'abstracting' an object, information about the object is 'thrown out' to produce a symbol</li>
        <ol class="hidden-section">
            <li>Individual objects as well as groups of objects and their relationships can be abstracted</li>
            <li>The geometry walls of a room can be abstracted as a set of linear shapes, with only lengths and angles</li>
        </ol><span class="see-more-button">See more</span>
        <li>Abstractions can be chained; symbols themselves can be abstracted further</li>
        <li>Abstractions can be also be incarnated</li>
        <ol class="hidden-section">
            <li>a symbol can be used to select transformations of the object domain such that the object domain now maps to the symbol</li>
            <ol class="hidden-section">
                <li>A blueprint can be used to build a house</li>
                <li>A textual description of a computer program can be turned into source code</li>
                <li>An image or a feeling can be used to guide the creation of a song or painting</li>
            </ol><span class="see-more-button">See more</span>
        </ol><span class="see-more-button">See more</span>
    </ol><span class="see-more-button">See more</span>
    <li>Absractions can also be applied to motions</li>
    <ol class="hidden-section">
        <li>Rather than causing a muscle to contract directly, an abstraction can represent contractions of muscle groups</li>
    </ol><span class="see-more-button">See more</span>
    <li>A loss function maps symbols to numerical scores.</li>
    <ol class="hidden-section">
        <li>A utility function can be see as a kind of loss function; the two are almost, but not quite equiavlent.</li>
        <ol class="hidden-section">
            <li>Ignoring hardware constraints, these two are identical</li>
            <ol class="hidden-section">
                <li>Maximizing x is mathematically equal to minizing the value of -x</li>
            </ol><span class="see-more-button">See more</span>
            <li>Given hardware constraints, there is a big difference</li>
            <ol class="hidden-section">
                <li>maximizing gain requires more effort as more gains are accomplished</li>
                <ol class="hidden-section">
                    <li>A utility function has the state of the entire universe as its input</li>
                    <li>Some portions can be considered irrelevant, but as utility increases, more needs to be modeled</li>
                    <ol class="hidden-section">
                        <li>A paperclip maximizer has to keep track of all the paperclips it has produced and where it has produced them and where they have been stored</li>
                        <li>It has to be very careful not to accidentally consume its own input</li>
                    </ol><span class="see-more-button">See more</span>
                </ol><span class="see-more-button">See more</span>
                <li>minimizing loss is cheaper than maximizing gain because _only_ the loss need be modeled</li>
                <ol class="hidden-section">
                    <li>An abstracted loss function essentially presumes some external _truth_ and attempts to measure only deviations from it</li>
                    <li>**This is far computationally cheaper than attempting to continuously model the state of the entire universe**</li>
                </ol><span class="see-more-button">See more</span>
            </ol><span class="see-more-button">See more</span>
        </ol><span class="see-more-button">See more</span>
    </ol><span class="see-more-button">See more</span>
    <li>An organism evolves using abstraction</li>
    <ol class="hidden-section">
        <li>by computing abstractions of itself</li>
        <ol class="hidden-section">
            <li>i.e. imagining ways it could be, or could act, or could move, or could communicate</li>
        </ol><span class="see-more-button">See more</span>
        <li>computing the loss function on these abstractions</li>
        <li>and acting to incarnate the lowest scoring abstraction</li>
    </ol><span class="see-more-button">See more</span>
</ol><span class="see-more-button">See more</span>
</ol><span class="see-more-button">See more</span>
<li>Abstraction functions can _also_ evolve</li>
<ol class="hidden-section">
<li>Because using abstraction has a cost and risk, abstraction functions themselves can evolve</li>
<ol class="hidden-section">
    <li>An architect might shift to a new style of blueprints which is faster to make and change</li>
    <li>The 'annual spring reorg' at Google</li>
    <li>Software engineers might refactor code to produce the same results in a way that's eaiser to change in the future</li>
    <li>Loss functions - because they are abstractions of the true environmental loss function - can evolve too</li>
    <ol class="hidden-section">
        <li>A business might develop new key performance indicators that it uses to determine the performance of its strategies</li>
    </ol><span class="see-more-button">See more</span>
</ol><span class="see-more-button">See more</span>
</ol><span class="see-more-button">See more</span>
<li>Using abstractions adds a new risk category</li>
<ol class="hidden-section">
<li>The fitness of an organism is determined over long periods of time by the environment itself</li>
<li>The fitness of a symbol is determined by the loss function</li>
<li>The loss function itself is an approximation of 'the true loss function' of the environment</li>
<ol class="hidden-section">
    <li>A program which fails to compile cannot make the business money</li>
    <li>A program which fails unit tests will likely not reliably make the business money, even if it's faster</li>
    <li>A program which passes all the unit tests, and performs some critical functions faster, in a domain where speed is rewarded, is more likely to make the business money</li>
    <li>A program which runs faster might not matter at all to the business and its development could thus represent waste</li>
</ol><span class="see-more-button">See more</span>
<li>Modifying a symbol in a way that reduces its computed loss might not incarnate an object whose _actual_ loss is lower.</li>
<ol class="hidden-section">
    <li>The loss function only an _approximation_ of the true loss function</li>
    <li>Releasing a software change is always risky even if it tests well</li>
    <li>A new marketing campaign might fail even if it did well in focus groups</li>
    <li>A strategy that performed well in simulated combat might not do well in actual battle</li>
</ol><span class="see-more-button">See more</span>
<li>Short term performance and even historical performance, are merely inputs to expected future performance</li>
<ol class="hidden-section">
    <li>A strategy that has worked well in the past, might have depended on transient environmental conditions</li>
</ol><span class="see-more-button">See more</span>
</ol><span class="see-more-button">See more</span>
</ol>
<h2>Hierarchically Composed Strategies Are Better Evolving Themselves</h2>
<ol class="section">
<li>A specific hierarchical structure maximizes the evolvability of conceptual networks</li>
<ol class="hidden-section">
<li>with the right kind of hierarchical structure, one component can be changed, or replaced entirely, with a limited 'blast radius'</li>
<ol class="hidden-section">
    <li>A 'parent' abstraction can remain identical, while the 'child' abstraction is repalced entirely</li>
    <li>This can work safely if the parent-child relationship is context-specific</li>
    <ol class="hidden-section">
        <li>i.e. the child concept has meaning (i.e. possibly contributes to motion of the body) only in a particular context</li>
        <li>e.g. Contingency Loci in bacteria</li>
        <ol class="hidden-section">
            <li>Ref: Adaptive evolution of highly mutable loci in pathogenic bacteria by Moxon, Rainey, Nowak nad Lenski</li>
        </ol><span class="see-more-button">See more</span>
    </ol><span class="see-more-button">See more</span>
    <li>This hierarchical configuration also **requires increasing abstraction at higher levels**</li>
    <ol class="hidden-section">
        <li>The x86 architecture can run anything</li>
        <li>An operating system will only run programs fitting a certain format, respecting certain syscalls.</li>
        <li>A browser will only run html + javascript + css (i.e. a specific subset of programs)</li>
        <li>A javascript web framework will only run spcific kinds of javascript objects</li>
        <li>A confirmation diaglog box expects a certain kind of application state to modify</li>
    </ol><span class="see-more-button">See more</span>
</ol><span class="see-more-button">See more</span>
<li>Absent the correct hierarchical structure, making discrete changes becomes impossible in large systems</li>
<ol class="hidden-section">
    <li>If a change anywhere in a concept network could effect behavior in any context, the risk of changes reducing fitness goes up</li>
    <ol class="hidden-section">
        <li> the loss function loses its approximative capacity</li>
    </ol><span class="see-more-button">See more</span>
    <li>One conseuqence of this is that **large systems with incorrectly defined hierarchies become incapable of change**</li>
    <li>The only way a large hierachy can continually evolve is if the top layers are extremely lightweight and flexible</li>
    <ol class="hidden-section">
        <li>The layers at the top need to be almost _unopinionated_ about the precise details what happens at the bottom</li>
        <li>instead they should focus primarily on **conflict resolution between intermediary layers**</li>
        <li>Otherwise, changes at the top will break the bottom in many ways, some of which are hard to recognize</li>
        <ol class="hidden-section">
            <li>Imagination is a computationally expensive, risky process that won't always go correctly</li>
        </ol><span class="see-more-button">See more</span>
    </ol><span class="see-more-button">See more</span>
</ol><span class="see-more-button">See more</span>
<li>Organisms that operate in multiple, distinct, changing environments will be selected for this hierarchical structure</li>
<ol class="hidden-section">
    <li>This conceptual structure allows for faster learning inindividual environments, thus faster adaptation to change</li>
    <li>This conceptual structure allows for generalization across environments, lowering the energy cost of learning</li>
</ol><span class="see-more-button">See more</span>
</ol><span class="see-more-button">See more</span>
<li>Memories, beliefs, and abstract concepts exist in a hierarchy of abstraction matching this shape</li>
<ol class="hidden-section">
<li>Each layer is a lossy compression of the layer below it</li>
<ol class="hidden-section">
    <li>Our senses give us impressions of the environment - they are abstraction functions computed on the environment</li>
    <li>Experiences are abstractions of groups of impressions</li>
    <li>Memories are abstractions of groups of experiences</li>
    <li>Concepts are abstractions of groups of memories</li>
    <li>Beliefs are abstractions of groups of concepts</li>
    <li>Principles are abstractions of groups of beliefs</li>
    <li>A cognitive structure with a single 'root' principle contains a single root abstraction</li>
</ol><span class="see-more-button">See more</span>
</ol><span class="see-more-button">See more</span>
<li>Learning patterns of reality produces a hierarchy of abstractions</li>
<ol class="hidden-section">
<li>Concepts themselves can evolve</li>
<ol class="hidden-section">
    <li>Recongizing similaries at one level of abstraction motivates higher levels of abstraction</li>
    <ol class="hidden-section">
        <li>A single experience of some new phenomena is contextualy localized</li>
        <li>Repeated experience of the same phenomena leads to multiple memories with similar properties</li>
        <li>A concept can be used to abstract the commonality between these experiences</li>
        <ol class="hidden-section">
            <li>The concept 'clock' links together mutiple experiences of seeing a round shape with markings on it</li>
            <li>Multiple classes with similar functionality might be refactored into a single abstract base class with shared methods</li>
        </ol><span class="see-more-button">See more</span>
    </ol><span class="see-more-button">See more</span>
    <li>A child may start thinking of a clock as being a circlular shape with markings, and end up with a more abstract representation that includes sundials and digital clocks</li>
    <li>An abstract concept can be unlearned as well</li>
    <ol class="hidden-section">
        <li>Falling to apply an abstract concept multiple times might lead to its breakdown</li>
        <ol class="hidden-section">
            <li>Believing that some process works in all cases, and failing to have it work in multiple cases, can kill the belief</li>
        </ol><span class="see-more-button">See more</span>
    </ol><span class="see-more-button">See more</span>
</ol><span class="see-more-button">See more</span>
<li>Pulling concepts up lowers their computational cost and increases their flexilibity</li>
<ol class="hidden-section">
    <li>Once an abstract concept has been learned, it can be applied in nother novel situations 'for free'</li>
    <ol class="hidden-section">
        <li>There is only the risk of mis-appling the concept</li>
        <li>The potential benefit is that learning in one situation or context can enable application of that same learning elsewhere</li>
        <li>Higher layers of the abstraction model are cheaper and easier to compute</li>
        <ol class="hidden-section">
            <li>Measuring the area of a wall is much easier to do accurately than simulatinng putting pain on it</li>
        </ol><span class="see-more-button">See more</span>
    </ol><span class="see-more-button">See more</span>
</ol><span class="see-more-button">See more</span>
<li>The limit of this process could be a single, general purpose, root concept</li>
<ol class="hidden-section">
    <li>Because abstractions are functions mapping large domains onto smaller ones, each layer of an abstraction hierarchy should be smaller and have fewer concepts than the one it generalizes</li>
    <li>The root concept might be seen as 'things which are always true'</li>
    <li>If this root concept _did_ exist and worked, it would be extremely useful becuase it would enable extremely generalized learning</li>
    <ol class="hidden-section">
        <li>It would also serve as a conflict-resolution mechanism</li>
    </ol><span class="see-more-button">See more</span>
    <li>If there were no recognized similaries between all instances of lower-level abstractions, this concept would not, or could not be learned</li>
</ol><span class="see-more-button">See more</span>
</ol><span class="see-more-button">See more</span>
</ol>
<h2>Abstract beliefs as organizing principles are necessary for group survival.</h2>
<h2>Trust in the organizing principle of the group is necessary for group cohesion</h2>
<h2>The evolutionary limit of "trust in the organizing principle" is trusting reality itself.</h2>
<h2>Trusting reality itself motivates faith, hope, and love as instrumental strategies.</h2>
</ol>
    <script>
        document.addEventListener('DOMContentLoaded', function () {
            const seeMoreButtons = document.querySelectorAll('.see-more-button');

            seeMoreButtons.forEach(button => {
                button.addEventListener('click', function () {
                    const hiddenSection = this.previousElementSibling;
                    if (hiddenSection && hiddenSection.classList.contains('hidden-section')) {
                        hiddenSection.classList.toggle('hidden-section');
                        this.textContent = hiddenSection.classList.contains('hidden-section') ? 'See more' : 'See less';
                    } else {
                        hiddenSection.classList.add('hidden-section');
                        this.textContent = 'See more';
                    }
                });
            });
        });
    </script>
</body>
</html>

