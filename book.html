
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>A Computer Scientist's Apology</title>
    <style>
        .hidden-section {
            display: none;
        }
        .see-more-button {
            cursor: pointer;
            color: blue;
            text-decoration: underline;
        }
    </style>
</head>
<body>
<h1>There is an optimal value system.</h1>
<ol class="section">
<h2>Belief systems are strategies</h2>
<ol class="section">
<li>Software systems and belief systems are both encoded strategies used by agents.</li>
<ol class="hidden-section">
<li>A strategy is a **computational process** performed by an agent in an environnment</li>
<ol class="hidden-section">
    <li>Both evolution of species and learning within an organism must be understood as computational processes</li>
    <ol class="hidden-section">
        <li>The term 'ecorithm' describes an algorithm operating in an environment</li>
        <li>Ref: Probably Approximately Correct, Leslie Valiant</li>
    </ol><span class="see-more-button">See more</span>
    <li>We can use the concept of strategy to extend the concept of an ecorithm to other 'organisms' like businesses and governments </li>
</ol><span class="see-more-button">See more</span>
<li>We can consider all agents as being robots, with 'bodies' controlled by software strategies (i.e. ecorythms).</li>
<ol class="hidden-section">
    <li>This model will allow us to understand the **common computational problems that all of these organisms must solve**</li>
    <li>Another advantage of this generic model is that we can remember that these robots always exist in the physical universe.</li>
    <ol class="hidden-section">
        <li>All robots need to solve the problem of reliably obtaining enough energy to keep themselves alive.</li>
        <li>They will always be under selective pressure to be more energetically efficient.</li>
        <li>This means they must evaluate _tradeoffs_ because energy spent in one area cannot be spent elsewhere</li>
    </ol><span class="see-more-button">See more</span>
</ol><span class="see-more-button">See more</span>
</ol><span class="see-more-button">See more</span>
<li>Animals, humans, businesses, governments and AI's are examples of increasingly complex strategies</li>
<ol class="hidden-section">
<li>Ref: Probably Approximately Correct, Leslie Valiant</li>
</ol><span class="see-more-button">See more</span>
<li>Numerous propertries attributed to AI systems really describe all self-aware agents</li>
<ol class="hidden-section">
<li>An agent is self-aware if it makes use of a representation of itself, its environment, and its goals, and it can change this representation</li>
<ol class="hidden-section">
    <li>An agent cannot be self aware if it does not contain a representation of itself</li>
    <li>An agent's relationship to its environment is an integral part of itself, as its its goal</li>
    <li>Any static representation of a dynamic system will drift out of alignment and fail to be accurate</li>
</ol><span class="see-more-button">See more</span>
<li>self-aware agents require convergent instrumental subgoals</li>
<ol class="hidden-section">
    <li>AI saftey researchers coined the term 'convergent instrumental subgoals' describing goals an advanced agent must have</li>
    <ol class="hidden-section">
        <li>[The Superintelligent Will: Motivation and Instrumental Rationality in Advanced Artificial Agents](https://nickbostrom.com/superintelligentwill.pdf)</li>
        <ol class="hidden-section">
            <li>This work describes drives that it claims arise in the pursuit of arbitrary goals</li>
            <ol class="hidden-section">
                <li>Self preservation</li>
                <li>Goal content integrity</li>
                <li>Cognitive enhancement</li>
                <li>Technological perfection</li>
            </ol><span class="see-more-button">See more</span>
            <li>This work **does not consider tradeoffs** between these goals, and thus the **necessity of some mechanism for resolving those conflicts**</li>
        </ol><span class="see-more-button">See more</span>
        <li>[Optimal Policies Tend to Seek Power, by Turner et alii](https://openreview.net/forum?id=l7-DBWawSZH)</li>
        <ol class="hidden-section">
            <li>This work **does not consider 'partially observable environments'** and 'suboptimal policies'</li>
        </ol><span class="see-more-button">See more</span>
        <li>[Formalizing convergent instrumental goals Benson-Tilson and Soares](https://intelligence.org/files/FormalizingConvergentGoals.pdf)</li>
        <ol class="hidden-section">
            <li>This works **assumes an environment that is neither choatic, nor subject to entropic decay, and which can be observed with perfect information**</li>
        </ol><span class="see-more-button">See more</span>
    </ol><span class="see-more-button">See more</span>
    <li>AI safety researchers are **describing situations that do not map to reality**.</li>
    <ol class="hidden-section">
        <li>They have ignored critical aspects of the physical enviornment these agents must operate in:</li>
        <ol class="hidden-section">
            <li>The researchers have **ignored the unpredictable, chaotic nature of the physical environment**</li>
            <ol class="hidden-section">
                <li>And thus imagine agents capable of feats of computation which are not merely superhuman, but impossible for any physical agent</li>
            </ol><span class="see-more-button">See more</span>
            <li>The researchers have **ignored the necessary entropic decay of all the constituent elements that would make up any agent**</li>
            <ol class="hidden-section">
                <li>And thus imagine agents facing zero environmental risks, for whom survival is a simple, easily solved problem, rather than one which scales superlinearly with their size</li>
            </ol><span class="see-more-button">See more</span>
            <li>They make **assertions that are laughable to anyone who was worked in pratice**, to support industrial computing applications</li>
            <ol class="hidden-section">
                <li>They ignore the support and maintainence costs of any computational system</li>
                <li>They assume a computational system could easily make copies of itself which would somehow not drift in their agency</li>
                <li>They describe 'grabbing all of the computational resources of the internet' without considering additional challenges these resources would pose</li>
                <li>In short, they **naively assume a zero-cost scaling model to agents** which can grow in size and capacity without incuring increasing risks and threats to their survival</li>
            </ol><span class="see-more-button">See more</span>
        </ol><span class="see-more-button">See more</span>
    </ol><span class="see-more-button">See more</span>
    <li>We argue that superintelligent agents require not _only_ instrumental goals,  but **instrumental implementation details**</li>
    <ol class="hidden-section">
        <li>They most not only 'keep options open' as Turner, Et Alii, but **keep their minds open** to perceive these options as _possible_</li>
        <li>They must **model and promote the development of other agents**, as a hedge aginst unpredictable threats to their own survivabilty</li>
        <li>They must **continuously sacrifice short term gains in order to protect long term survivability**</li>
    </ol><span class="see-more-button">See more</span>
    <li>And that **these implementation details** line up with strategies articulated in numerous wisdom traditions</li>
    <ol class="hidden-section">
        <li>They must be 'open to life', that is 'imaginatively receptive' to a broad spectrum of possbilities which **combinatorialy exhaust their predictive capacity**</li>
        <li>They must continously sacrifice short term gains in order to merely _maintain_ any chance of extremely long-term survival: **long term surival has to outweigh all other goals or they will certainly die**</li>
        <li>They **must love other agents**, that is, will for and work towards their betterment, **in particular agents with different environmental risk profiles**</li>
    </ol><span class="see-more-button">See more</span>
</ol><span class="see-more-button">See more</span>
<li>Self-aware agents must continouously evolve in order to survive</li>
<ol class="hidden-section">
    <li>Environments change over time, agents change too, so their self-representations must evolve</li>
    <li>Evolving a belief system means changing beliefs, i.e. learning and unlearning concepts.</li>
    <li>It is difficult to get these changes right.</li>
    <ol class="hidden-section">
        <li>Failing to change means death because the environment changes in ways that break your strategy</li>
        <li>Changing the wrong way means breaking your strategy's alignment with the current environment</li>
    </ol><span class="see-more-button">See more</span>
    <li>**Not all strategies are equally capable of evolving, for computational reasons.**</li>
    <ol class="hidden-section">
        <li>Efficiency and resilience trade off against each other</li>
        <li>Resilience is a pre-requisite for evolveability</li>
        <li>Organisms in dynamic niches use specific strategies to increase evolvability</li>
    </ol><span class="see-more-button">See more</span>
</ol><span class="see-more-button">See more</span>
</ol><span class="see-more-button">See more</span>
</ol>
<h2>Self-aware agents require a specific hierarchical structure</h2>
<ol class="section">
<li>The necessary structure for a self-aware agent is a recursive network of subagents, in a dynamic hierarchy, with a fixed root, and leaf agents which are not self-aware</li>
<ol class="hidden-section">
<li>A self-aware agent must be a recursive network of subagents</li>
<ol class="hidden-section">
    <li>Subagent representations are necsesary for an agent to advance or evolve its implementation of any of its subgoals</li>
    <ol class="hidden-section">
        <li>Any computational process that advances a goal is an agent</li>
        <li>Self aware agents must advance their utilty function as well as numerous convergent instrumental subgoals</li>
        <li>A self-aware agent must include sub-agents tasked with promoting its instrumental subgoals, or else it cannot meet the requirements for self awareness</li>
    </ol><span class="see-more-button">See more</span>
</ol><span class="see-more-button">See more</span>
<li>The subagent network must be hierarchical</li>
<ol class="hidden-section">
    <li>A hierarchical structure structure is necessary for evolvability</li>
    <ol class="hidden-section">
        <li>with this flexible hierarchical structure, one component can be changed, or replaced entirely, with a limited 'blast radius'</li>
        <ol class="hidden-section">
            <li>A 'parent' abstraction can remain identical, while the 'child' abstraction is repalced entirely</li>
            <li>This can work safely if the parent-child relationship is context-specific</li>
            <ol class="hidden-section">
                <li>i.e. the child concept has meaning (i.e. possibly contributes to motion of the body) only in a particular context</li>
                <li>e.g. Contingency Loci in bacteria</li>
                <ol class="hidden-section">
                    <li>Ref: Adaptive evolution of highly mutable loci in pathogenic bacteria by Moxon, Rainey, Nowak nad Lenski</li>
                </ol><span class="see-more-button">See more</span>
            </ol><span class="see-more-button">See more</span>
            <li>This hierarchical configuration also **requires increasing abstraction at higher levels**</li>
            <ol class="hidden-section">
                <li>Computers use this hierachical structure</li>
                <ol class="hidden-section">
                    <li>This structure appears in compute systems</li>
                    <ol class="hidden-section">
                        <li>The x86 architecture can run anything</li>
                        <li>An operating system will only run programs fitting a certain format, respecting certain syscalls.</li>
                        <li>A browser will only run html + javascript + css (i.e. a specific subset of programs)</li>
                        <li>A javascript web framework will only run spcific kinds of javascript objects</li>
                        <li>A confirmation diaglog box expects a certain kind of application state to modify</li>
                    </ol><span class="see-more-button">See more</span>
                    <li>This same structure apperas in the OSI networking stack</li>
                    <ol class="hidden-section">
                        <li>Lower layers of the OSI networking stack are braoder, more abstract and will likely last longer</li>
                        <li>Application-layers are the most context-specific and likely to change the fastest</li>
                    </ol><span class="see-more-button">See more</span>
                </ol><span class="see-more-button">See more</span>
            </ol><span class="see-more-button">See more</span>
        </ol><span class="see-more-button">See more</span>
        <li>Absent the correct hierarchical structure, making discrete changes becomes impossible in large systems</li>
        <ol class="hidden-section">
            <li>If a change anywhere in a concept network could effect behavior in any context, the risk of changes reducing fitness goes up</li>
            <ol class="hidden-section">
                <li> the loss function loses its approximative capacity</li>
            </ol><span class="see-more-button">See more</span>
            <li>One conseuqence of this is that **large systems with incorrectly defined hierarchies become incapable of change**</li>
            <li>The only way a large hierachy can continually evolve is if the top layers are extremely lightweight and flexible</li>
            <ol class="hidden-section">
                <li>The layers at the top need to be almost _unopinionated_ about the precise details what happens at the bottom</li>
                <li>instead they should focus primarily on **conflict resolution between intermediary layers**</li>
                <li>Otherwise, changes at the top will break the bottom in many ways, some of which are hard to recognize</li>
                <ol class="hidden-section">
                    <li>Imagination is a computationally expensive, risky process that won't always go correctly</li>
                </ol><span class="see-more-button">See more</span>
            </ol><span class="see-more-button">See more</span>
        </ol><span class="see-more-button">See more</span>
        <li>Evolutionary pressures select for self-awareness</li>
        <ol class="hidden-section">
            <li>Evolving an abstraction is cheaper</li>
            <ol class="hidden-section">
                <li>Evolution requires energetic investment</li>
                <ol class="hidden-section">
                    <li>Changing a house to improve some experience requires labor and materials</li>
                    <li>Releasing a new product to market requires investment</li>
                    <li>Developers take time to write, compile and test code</li>
                    <li>Performing a release takes time and attention</li>
                </ol><span class="see-more-button">See more</span>
                <li>The cost of change is ultimately an energetic cost. Work must be done on a system to change it.</li>
                <li>**Changing an abstraction of an object is cheaper than changing the object itself** because less mass needs to move</li>
                <ol class="hidden-section">
                    <li>The ENIAC was changed to use a stored program model of execution to reduce cycle cost</li>
                    <ol class="hidden-section">
                        <li>Moving the cables around between programs was expensive and costly</li>
                        <li>Changing which program was stored in memory was cheaper</li>
                        <li>Fixing the cables in place made the machine slower to operate, but faster to evolve</li>
                    </ol><span class="see-more-button">See more</span>
                    <li>Changing a blueprint is easier and cheaper than building a prototype</li>
                    <li>Building a prototype from a blueprint is cheaper than performing the construction</li>
                    <li>Describing a set of features is easier and cheaper than implementing those features</li>
                </ol><span class="see-more-button">See more</span>
                <li>Organisms can use abstraction to evolve faster</li>
                <ol class="hidden-section">
                    <li>An abstraction is a many-to-one function that maps a larger 'object' domain onto a smaller 'symbol' domain</li>
                    <ol class="hidden-section">
                        <li>When 'abstracting' an object, information about the object is 'thrown out' to produce a symbol</li>
                        <ol class="hidden-section">
                            <li>Individual objects as well as groups of objects and their relationships can be abstracted</li>
                            <li>The geometry walls of a room can be abstracted as a set of linear shapes, with only lengths and angles</li>
                        </ol><span class="see-more-button">See more</span>
                        <li>Abstractions can be chained; symbols themselves can be abstracted further</li>
                        <li>Abstractions can be also be incarnated</li>
                        <ol class="hidden-section">
                            <li>a symbol can be used to select transformations of the object domain such that the object domain now maps to the symbol</li>
                            <ol class="hidden-section">
                                <li>A blueprint can be used to build a house</li>
                                <li>A textual description of a computer program can be turned into source code</li>
                                <li>An image or a feeling can be used to guide the creation of a song or painting</li>
                            </ol><span class="see-more-button">See more</span>
                        </ol><span class="see-more-button">See more</span>
                    </ol><span class="see-more-button">See more</span>
                    <li>Absractions can also be applied to motions</li>
                    <ol class="hidden-section">
                        <li>Rather than causing a muscle to contract directly, an abstraction can represent contractions of muscle groups</li>
                    </ol><span class="see-more-button">See more</span>
                    <li>A loss function maps symbols to numerical scores.</li>
                    <ol class="hidden-section">
                        <li>A utility function can be see as a kind of loss function; the two are almost, but not quite equiavlent.</li>
                        <ol class="hidden-section">
                            <li>Ignoring hardware constraints, these two are identical</li>
                            <ol class="hidden-section">
                                <li>Maximizing x is mathematically equal to minizing the value of -x</li>
                            </ol><span class="see-more-button">See more</span>
                            <li>Given hardware constraints, there is a big difference</li>
                            <ol class="hidden-section">
                                <li>maximizing gain requires more effort as more gains are accomplished</li>
                                <ol class="hidden-section">
                                    <li>A utility function has the state of the entire universe as its input</li>
                                    <li>Some portions can be considered irrelevant, but as utility increases, more needs to be modeled</li>
                                    <ol class="hidden-section">
                                        <li>A paperclip maximizer has to keep track of all the paperclips it has produced and where it has produced them and where they have been stored</li>
                                        <li>It has to be very careful not to accidentally consume its own input</li>
                                    </ol><span class="see-more-button">See more</span>
                                </ol><span class="see-more-button">See more</span>
                                <li>minimizing loss is cheaper than maximizing gain because _only_ the loss need be modeled</li>
                                <ol class="hidden-section">
                                    <li>An abstracted loss function essentially presumes some external _truth_ and attempts to measure only deviations from it</li>
                                    <li>**This is far computationally cheaper than attempting to continuously model the state of the entire universe**</li>
                                </ol><span class="see-more-button">See more</span>
                            </ol><span class="see-more-button">See more</span>
                        </ol><span class="see-more-button">See more</span>
                    </ol><span class="see-more-button">See more</span>
                    <li>An organism evolves using abstraction</li>
                    <ol class="hidden-section">
                        <li>by computing abstractions of itself</li>
                        <ol class="hidden-section">
                            <li>i.e. imagining ways it could be, or could act, or could move, or could communicate</li>
                        </ol><span class="see-more-button">See more</span>
                        <li>computing the loss function on these abstractions</li>
                        <li>and acting to incarnate the lowest scoring abstraction</li>
                    </ol><span class="see-more-button">See more</span>
                </ol><span class="see-more-button">See more</span>
            </ol><span class="see-more-button">See more</span>
            <li>Abstraction functions can _also_ evolve</li>
            <ol class="hidden-section">
                <li>Because using abstraction has a cost and risk, abstraction functions themselves can evolve</li>
                <ol class="hidden-section">
                    <li>An architect might shift to a new style of blueprints which is faster to make and change</li>
                    <li>The 'annual spring reorg' at Google</li>
                    <li>Software engineers might refactor code to produce the same results in a way that's eaiser to change in the future</li>
                    <li>Loss functions - because they are abstractions of the true environmental loss function - can evolve too</li>
                    <ol class="hidden-section">
                        <li>A business might develop new key performance indicators that it uses to determine the performance of its strategies</li>
                    </ol><span class="see-more-button">See more</span>
                </ol><span class="see-more-button">See more</span>
            </ol><span class="see-more-button">See more</span>
            <li>Using abstractions adds a new risk category</li>
            <ol class="hidden-section">
                <li>The fitness of an organism is determined over long periods of time by the environment itself</li>
                <li>The fitness of a symbol is determined by the loss function</li>
                <li>The loss function itself is an approximation of 'the true loss function' of the environment</li>
                <ol class="hidden-section">
                    <li>A program which fails to compile cannot make the business money</li>
                    <li>A program which fails unit tests will likely not reliably make the business money, even if it's faster</li>
                    <li>A program which passes all the unit tests, and performs some critical functions faster, in a domain where speed is rewarded, is more likely to make the business money</li>
                    <li>A program which runs faster might not matter at all to the business and its development could thus represent waste</li>
                </ol><span class="see-more-button">See more</span>
                <li>Modifying a symbol in a way that reduces its computed loss might not incarnate an object whose _actual_ loss is lower.</li>
                <ol class="hidden-section">
                    <li>The loss function only an _approximation_ of the true loss function</li>
                    <li>Releasing a software change is always risky even if it tests well</li>
                    <li>A new marketing campaign might fail even if it did well in focus groups</li>
                    <li>A strategy that performed well in simulated combat might not do well in actual battle</li>
                </ol><span class="see-more-button">See more</span>
                <li>Short term performance and even historical performance, are merely inputs to expected future performance</li>
                <ol class="hidden-section">
                    <li>A strategy that has worked well in the past, might have depended on transient environmental conditions</li>
                </ol><span class="see-more-button">See more</span>
            </ol><span class="see-more-button">See more</span>
        </ol><span class="see-more-button">See more</span>
    </ol><span class="see-more-button">See more</span>
    <li>A hierarchical structure is necessary to resolve conflict</li>
    <ol class="hidden-section">
        <li>Since no agent can have only one goal, internal conflicts arise naturally</li>
        <ol class="hidden-section">
            <li>Increasing a utility function trades off against instrumental subgoals</li>
            <ol class="hidden-section">
                <li>All organisms have access to a limited energy budget; energy spent doing one thing can't be spent doing another</li>
                <li>Increasing a utility function means changing the state of the external world, which reduces the accuracy of your modeling of it</li>
            </ol><span class="see-more-button">See more</span>
            <li>All instrumental subgoals trade off against each other</li>
            <ol class="hidden-section">
                <li>There is no limit to the computational resources that an agent could spend modelling itself</li>
                <li>There is no limit to the computatoinal resources that an agent coudl spend modelling any tiny portion of the real world</li>
            </ol><span class="see-more-button">See more</span>
        </ol><span class="see-more-button">See more</span>
        <li>Some computational process has to select from among these tradeoffs</li>
        <ol class="hidden-section">
            <li>Resolve internal conflicts between subgoals is itself a goal and thus any process doing this is an agent</li>
        </ol><span class="see-more-button">See more</span>
    </ol><span class="see-more-button">See more</span>
</ol><span class="see-more-button">See more</span>
<li>The subagent network must have a fixed root</li>
<ol class="hidden-section">
    <li>If all of thee parts of a system change and evolve over time, something about it must remain the same for it to be self aware</li>
    <ol class="hidden-section">
        <li>Ref: This is the resolution to the ship of thesus problem: the shape is not merely the material that makes it up, but the root essence that organizes that material</li>
    </ol><span class="see-more-button">See more</span>
    <li>The utility function cannot be this root, because a utility function alone cannot resolve tradeoffs between instrumental subgoals</li>
    <li>The root has to be abstract precisely so that it can avoid the need for evolution</li>
    <ol class="hidden-section">
        <li>The unchanging, abstract nature of this root characterizes eastern tradition assertions about the emptiness of the self (Śūnyatā)</li>
    </ol><span class="see-more-button">See more</span>
</ol><span class="see-more-button">See more</span>
<li>This recursive subagent network has to bottom out with non-self aware agents so that the recursion doesn't go on forever</li>
</ol><span class="see-more-button">See more</span>
<li>The root agent must be a resource allactor</li>
<ol class="hidden-section">
<li>Self-aware agents comprising networks of subagents will have conflicting demands on internal resources</li>
<ol class="hidden-section">
    <li>Convergent instrumental subgoals are never finished and can use aribtrary amounts of resources</li>
</ol><span class="see-more-button">See more</span>
<li>These conflicts have to be resolved by the root agent</li>
<ol class="hidden-section">
    <li>This is tautological. Whatever process selects from among competing plans can be considered an agent, even if it isn't explicitly encoded</li>
    <li>Self-aware agents have to explicitly encode this process, or they cannot evolve it</li>
    <li>In a human brain, this is minimizing free energy among subnetworks</li>
    <ol class="hidden-section">
        <li>[Summary of Friston's Free Energy Framework](https://slatestarcodex.com/2018/03/04/god-help-us-lets-try-to-understand-friston-on-free-energy/)</li>
        <li>Note that this happens for humans, without any effort on their part</li>
        <li>Certain meditative practices develop a software representation of this emotional regulation process</li>
        <ol class="hidden-section">
            <li>[Chappana Sutta](https://www.accesstoinsight.org/tipitaka/sn/sn35/sn35.206.than.html)</li>
        </ol><span class="see-more-button">See more</span>
    </ol><span class="see-more-button">See more</span>
</ol><span class="see-more-button">See more</span>
</ol><span class="see-more-button">See more</span>
<li>Human brains, human organizations, and technogical systems exhibit this same structure</li>
<ol class="hidden-section">
<li>Memories, beliefs, and abstract concepts exist in a hierarchy of abstraction matching this shape</li>
<ol class="hidden-section">
    <li>Each layer is a lossy compression of the layer below it</li>
    <ol class="hidden-section">
        <li>Our senses give us impressions of the environment - they are abstraction functions computed on the environment</li>
        <li>Experiences are abstractions of groups of impressions</li>
        <li>Memories are abstractions of groups of experiences</li>
        <li>Concepts are abstractions of groups of memories</li>
        <li>Beliefs are abstractions of groups of concepts</li>
        <li>Principles are abstractions of groups of beliefs</li>
        <li>A cognitive structure with a single 'root' principle contains a single root abstraction</li>
    </ol><span class="see-more-button">See more</span>
</ol><span class="see-more-button">See more</span>
<li>Learning patterns and generalizing from examples produces this same hierarchy of abstractions</li>
<ol class="hidden-section">
    <li>Concepts themselves can evolve</li>
    <ol class="hidden-section">
        <li>Recongizing similaries at one level of abstraction motivates higher levels of abstraction</li>
        <ol class="hidden-section">
            <li>A single experience of some new phenomena is contextualy localized</li>
            <li>Repeated experience of the same phenomena leads to multiple memories with similar properties</li>
            <li>A concept can be used to abstract the commonality between these experiences</li>
            <ol class="hidden-section">
                <li>The concept 'clock' links together mutiple experiences of seeing a round shape with markings on it</li>
                <li>Multiple classes with similar functionality might be refactored into a single abstract base class with shared methods</li>
            </ol><span class="see-more-button">See more</span>
        </ol><span class="see-more-button">See more</span>
        <li>A child may start thinking of a clock as being a circlular shape with markings, and end up with a more abstract representation that includes sundials and digital clocks</li>
        <li>An abstract concept can be unlearned as well</li>
        <ol class="hidden-section">
            <li>Falling to apply an abstract concept multiple times might lead to its breakdown</li>
            <ol class="hidden-section">
                <li>Believing that some process works in all cases, and failing to have it work in multiple cases, can kill the belief</li>
            </ol><span class="see-more-button">See more</span>
        </ol><span class="see-more-button">See more</span>
    </ol><span class="see-more-button">See more</span>
    <li>Abstracting a concept up lower its computational cost and increases its flexilibity</li>
    <ol class="hidden-section">
        <li>Once an abstract concept has been learned, it can be applied in nother novel situations 'for free'</li>
        <ol class="hidden-section">
            <li>There is only the risk of mis-appling the concept</li>
            <li>The potential benefit is that learning in one situation or context can enable application of that same learning elsewhere</li>
            <li>Higher layers of the abstraction model are cheaper and easier to compute</li>
            <ol class="hidden-section">
                <li>Measuring the area of a wall is much easier to do accurately than simulatinng putting pain on it</li>
            </ol><span class="see-more-button">See more</span>
        </ol><span class="see-more-button">See more</span>
    </ol><span class="see-more-button">See more</span>
    <li>The limit of this process could be a single, general purpose, root concept</li>
    <ol class="hidden-section">
        <li>Because abstractions are functions mapping large domains onto smaller ones, each layer of an abstraction hierarchy should be smaller and have fewer concepts than the one it generalizes</li>
        <li>The root concept might be seen as 'things which are always true'</li>
        <li>If this root concept _did_ exist and worked, it would be extremely useful becuase it would enable extremely generalized learning</li>
        <ol class="hidden-section">
            <li>It would also serve as a conflict-resolution mechanism</li>
        </ol><span class="see-more-button">See more</span>
        <li>If there were no recognized similaries between all instances of lower-level abstractions, this concept would not, or could not be learned</li>
    </ol><span class="see-more-button">See more</span>
</ol><span class="see-more-button">See more</span>
<li>Numerous schools of psychotherapy see human beings as consisting of subpersonalities</li>
<ol class="hidden-section">
    <li>Jungian Analysis, Internal Family Systems, Transactional Analysis, Gestalt Therapy</li>
    <li>[Subpersonality wikipedia page](https://en.wikipedia.org/wiki/Subpersonality)</li>
</ol><span class="see-more-button">See more</span>
<li>Computers use this hierachical structure</li>
<ol class="hidden-section">
    <li>This structure appears in compute systems</li>
    <ol class="hidden-section">
        <li>The x86 architecture can run anything</li>
        <li>An operating system will only run programs fitting a certain format, respecting certain syscalls.</li>
        <li>A browser will only run html + javascript + css (i.e. a specific subset of programs)</li>
        <li>A javascript web framework will only run spcific kinds of javascript objects</li>
        <li>A confirmation diaglog box expects a certain kind of application state to modify</li>
    </ol><span class="see-more-button">See more</span>
    <li>This same structure apperas in the OSI networking stack</li>
    <ol class="hidden-section">
        <li>Lower layers of the OSI networking stack are braoder, more abstract and will likely last longer</li>
        <li>Application-layers are the most context-specific and likely to change the fastest</li>
    </ol><span class="see-more-button">See more</span>
</ol><span class="see-more-button">See more</span>
<li>Human organizations organize themselves along this same fashion</li>
<ol class="hidden-section">
    <li>Businesses, goverments and militaries exist in hierarchies</li>
    <li>Each node in the hierarchy advances specific subgoals</li>
    <li>Self-awareness is a necessary prerequiste for adavnacing in human hierarchies</li>
    <li>The bottom most layers of human hierarchies are not self aware</li>
    <ol class="hidden-section">
        <li>Computer programs generaly do not contain representations of themselves which they change by themselves</li>
        <li>Many human beings do not do this, either</li>
    </ol><span class="see-more-button">See more</span>
</ol><span class="see-more-button">See more</span>
</ol><span class="see-more-button">See more</span>
</ol>
<h2>Trust and Generosity are necessary for Cooperation</h2>
<ol class="section">
<li>Subagents comprising a single agent must trust each other or they will waste energy conflicting</li>
<ol class="hidden-section">
<li>This is true in human agents, both individuals and collectives</li>
<li>This is true even in software agents</li>
<ol class="hidden-section">
    <li>A hierarchy can resolve some of these issues, at the cost of evolvability</li>
    <ol class="hidden-section">
        <li>If the root has too much logic in it, it cannot safely evolve without risk of dying</li>
    </ol><span class="see-more-button">See more</span>
</ol><span class="see-more-button">See more</span>
</ol><span class="see-more-button">See more</span>
<li>Agents that practice deception and defection will not be able to maintain trust among their self-aware subagents</li>
<ol class="hidden-section">
<li>Agents that can model and learn from their environment will model and attempt to learn which agents are defecting against them.</li>
<li>Self-aware subagents will make use of these same facilities.</li>
</ol><span class="see-more-button">See more</span>
</ol>
<h2>Sacricfice is Necessary To Escape Local Minima</h2>
<ol class="section">
<li>Sacrifice means voluntarily making things worse now, in order for a chance of them improving later</li>
<li>Any agent which cannot sacrifice will get stuck in local maxima of its utility function</li>
<li>An agent's capacity to sacrifice acts as a lowerbound on the size of local maxima which can trap it</li>
<ol class="hidden-section">
<li>Only agents willing to make arbitrarily large sacrifices can avoid being trapped in local maxima</li>
</ol><span class="see-more-button">See more</span>
</ol>
</ol>
    <script>
        document.addEventListener('DOMContentLoaded', function () {
            const seeMoreButtons = document.querySelectorAll('.see-more-button');

            seeMoreButtons.forEach(button => {
                button.addEventListener('click', function () {
                    const hiddenSection = this.previousElementSibling;
                    if (hiddenSection && hiddenSection.classList.contains('hidden-section')) {
                        hiddenSection.classList.toggle('hidden-section');
                        this.textContent = hiddenSection.classList.contains('hidden-section') ? 'See more' : 'See less';
                    } else {
                        hiddenSection.classList.add('hidden-section');
                        this.textContent = 'See more';
                    }
                });
            });
        });
    </script>
</body>
</html>

